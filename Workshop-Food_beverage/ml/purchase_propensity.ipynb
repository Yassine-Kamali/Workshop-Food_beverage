{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f3794b",
   "metadata": {},
   "source": [
    "# Modèle de Propension à l'Achat\n",
    "\n",
    "Ce notebook développe un modèle d'apprentissage automatique pour prédire la propension des clients à effectuer des achats.\n",
    "\n",
    "## Objectifs :\n",
    "- Analyser le comportement d'achat des clients\n",
    "- Construire un modèle prédictif de propension à l'achat\n",
    "- Identifier les clients à forte propension\n",
    "- Fournir des insights pour le marketing ciblé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import pd_read_sql\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Bibliothèques importées avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowflake.connector\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Bibliothèques importées avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec708b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se connecter à Snowflake\n",
    "conn_params = {\n",
    "    'user': 'workshop_user',\n",
    "    'password': 'VotreMotDePasse123!',\n",
    "    'account': 'dnb65599',\n",
    "    'warehouse': 'ANYCOMPANY_WH',\n",
    "    'database': 'ANYCOMPANY_LAB',\n",
    "    'schema': 'ANALYTICS'\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**conn_params)\n",
    "print(\"Connecté à Snowflake !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82396296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement des données\n",
    "# Features pour la prédiction\n",
    "features = ['age', 'income_category', 'region', 'total_transactions', \n",
    "           'total_amount', 'avg_transaction_amount', 'days_since_last_purchase',\n",
    "           'purchase_frequency', 'avg_days_between_purchases']\n",
    "\n",
    "# Target : propension à l'achat (basé sur la fréquence et le montant récent)\n",
    "target = 'high_propensity_customer'\n",
    "\n",
    "# Préparer les données\n",
    "X = df[features].fillna(0)\n",
    "y = df[target]\n",
    "\n",
    "# Encoder les variables catégorielles\n",
    "X = pd.get_dummies(X, columns=['income_category', 'region'], drop_first=True)\n",
    "\n",
    "# Mettre à l'échelle les features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Features : {list(X.columns)}\")\n",
    "print(f\"Target : {target}\")\n",
    "print(f\"Distribution des classes : {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Ensemble d'entraînement : {X_train.shape[0]} échantillons\")\n",
    "print(f\"Ensemble de test : {X_test.shape[0]} échantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b50a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modèle 1 : Régression Logistique\n",
    "lr_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Modèle 2 : Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Modèles entraînés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c861c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles\n",
    "models = {'Régression Logistique': lr_model, 'Random Forest': rf_model}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Rapport de Classification :\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Score ROC AUC : {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Propension Faible', 'Propension Élevée'],\n",
    "                yticklabels=['Propension Faible', 'Propension Élevée'])\n",
    "    plt.title(f'Matrice de Confusion - {name}')\n",
    "    plt.ylabel('Réel')\n",
    "    plt.xlabel('Prédit')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('Taux de Faux Positifs')\n",
    "plt.ylabel('Taux de Vrais Positifs')\n",
    "plt.title('Courbes ROC - Comparaison des Modèles')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features (Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Features les Plus Importantes (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 features les plus importantes :\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29210b57",
   "metadata": {},
   "source": [
    "## Insights Business et Recommandations\n",
    "\n",
    "### Principaux Résultats :\n",
    "1. **Facteurs de Propension** : [Analyse des features importantes]\n",
    "2. **Performance des Modèles** : [Comparaison des scores]\n",
    "3. **Segments Clients** : [Identification des groupes à forte propension]\n",
    "\n",
    "### Recommandations :\n",
    "1. **Ciblage Marketing** : Se concentrer sur les clients identifiés à forte propension\n",
    "2. **Stratégies de Fidélisation** : Programmes pour les clients à risque\n",
    "3. **Campagnes Personnalisées** : Messages adaptés aux profils clients\n",
    "4. **Timing Optimal** : Moments stratégiques pour les communications\n",
    "\n",
    "### Prochaines Étapes :\n",
    "- Déployer le modèle sélectionné en production\n",
    "- Intégrer les prédictions dans le système de CRM\n",
    "- Mesurer l'impact des campagnes ciblées\n",
    "- Mettre à jour le modèle avec de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd98d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer la connexion\n",
    "conn.close()\n",
    "print(\"Analyse terminée et connexion fermée !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3d66e3",
   "metadata": {},
   "source": [
    "# Modèle de Propension à l'Achat\n",
    "\n",
    "Ce notebook développe un modèle pour prédire la propension des clients à effectuer des achats futurs basé sur des features démographiques et comportementales.\n",
    "\n",
    "## Objectifs :\n",
    "- Prédire la probabilité d'achats futurs\n",
    "- Identifier les principaux drivers du comportement d'achat\n",
    "- Segmenter les clients selon leur potentiel d'achat\n",
    "- Fournir des recommandations de ciblage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a9cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import pd_read_sql\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Bibliothèques importées avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50480f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se connecter à Snowflake\n",
    "conn_params = {\n",
    "    'user': 'workshop_user',\n",
    "    'password': 'VotreMotDePasse123!',\n",
    "    'account': 'dnb65599',\n",
    "    'warehouse': 'ANYCOMPANY_WH',\n",
    "    'database': 'ANYCOMPANY_LAB',\n",
    "    'schema': 'ANALYTICS'\n",
    "}\n",
    "\n",
    "conn = snowflake.connector.connect(**conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a26f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données clients avec target synthétique\n",
    "# Note : En scénario réel, le target serait basé sur l'historique d'achat réel\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    *,\n",
    "    -- Target synthétique : propension élevée si haut revenu et âge jeune\n",
    "    CASE WHEN annual_income > 60000 AND age < 50 THEN 1 ELSE 0 END AS purchase_propensity\n",
    "FROM ANALYTICS.customer_ml_features\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, conn)\n",
    "print(f\"{len(df)} enregistrements clients chargés\")\n",
    "print(f\"Distribution du target : {df['purchase_propensity'].value_counts(normalize=True)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd82406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des features\n",
    "features = ['age', 'annual_income', 'age_group_encoded', 'income_segment_encoded',\n",
    "           'region_north', 'region_south', 'region_east', 'region_west']\n",
    "\n",
    "X = df[features]\n",
    "y = df['purchase_propensity']\n",
    "\n",
    "# Mettre à l'échelle les features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Features sélectionnées : {features}\")\n",
    "print(f\"Forme de la matrice de features : {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diviser les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Échantillons d'entraînement : {len(X_train)}\")\n",
    "print(f\"Échantillons de test : {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d787f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle de Régression Logistique\n",
    "lr_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Régression Logistique entraînée !\")\n",
    "print(f\"ROC AUC : {roc_auc_score(y_test, y_pred_proba_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89376f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest entraîné !\")\n",
    "print(f\"ROC AUC : {roc_auc_score(y_test, y_pred_proba_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a5aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des modèles\n",
    "models = ['Régression Logistique', 'Random Forest']\n",
    "predictions = [y_pred_lr, y_pred_rf]\n",
    "probabilities = [y_pred_proba_lr, y_pred_proba_rf]\n",
    "\n",
    "for name, pred, proba in zip(models, predictions, probabilities):\n",
    "    print(f\"\\n{name} :\")\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(f\"ROC AUC : {roc_auc_score(y_test, proba):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b52276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, proba in zip(models, probabilities):\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Aléatoire')\n",
    "plt.xlabel('Taux de Faux Positifs')\n",
    "plt.ylabel('Taux de Vrais Positifs')\n",
    "plt.title('Courbes ROC - Modèles de Propension à l\\'Achat')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781175a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des features (Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Importance des Features - Propension à l\\'Achat')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "print(\"Features principales driving la propension à l'achat :\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Générer des scores de propension pour tous les clients\n",
    "df['propensity_score'] = rf_model.predict_proba(scaler.transform(df[features]))[:, 1]\n",
    "\n",
    "# Créer des segments de propension\n",
    "df['propensity_segment'] = pd.qcut(df['propensity_score'], q=4, labels=['Faible', 'Moyen', 'Élevé', 'Très Élevé'])\n",
    "\n",
    "print(\"Distribution des scores de propension :\")\n",
    "print(df['propensity_segment'].value_counts().sort_index())\n",
    "\n",
    "# Visualiser la distribution de propension\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='propensity_score', hue='propensity_segment', multiple='stack')\n",
    "plt.title('Distribution de la Propension à l\\'Achat Client')\n",
    "plt.xlabel('Score de Propension')\n",
    "plt.ylabel('Nombre de Clients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc27898",
   "metadata": {},
   "source": [
    "## Recommandations Business\n",
    "\n",
    "### Insights Clés :\n",
    "1. **Drivers Principaux** : Le revenu et l'âge sont les principaux drivers de la propension à l'achat\n",
    "2. **Segments à Haute Propension** : Se concentrer sur les efforts marketing vers les clients à haute propension\n",
    "3. **Performance du Modèle** : [Scores ROC AUC]\n",
    "\n",
    "### Recommandations Actionnables :\n",
    "1. **Marketing Ciblé** : Prioriser les clients à haute propension pour les campagnes\n",
    "2. **Personnalisation** : Adapter les offres basées sur les scores de propension\n",
    "3. **Focus Fidélisation** : Développer des stratégies de rétention pour les segments à haute propension\n",
    "4. **Stratégie d'Acquisition** : Cibler des profils similaires pour l'acquisition de clients\n",
    "\n",
    "### Mise en Œuvre :\n",
    "- Intégrer les scores de propension dans le CRM\n",
    "- Utiliser les scores pour le ciblage des campagnes\n",
    "- Surveiller les changements de score dans le temps\n",
    "- Mettre à jour le modèle avec de nouvelles données tous les trimestres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c33dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les résultats\n",
    "results_df = df[['customer_id', 'propensity_score', 'propensity_segment']]\n",
    "print(f\"Résultats prêts : {len(results_df)} scores de propension client\")\n",
    "\n",
    "# Fermer la connexion\n",
    "conn.close()\n",
    "print(\"Analyse de propension à l'achat terminée !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163e9823",
   "metadata": {},
   "source": [
    "# Purchase Propensity Model\n",
    "\n",
    "This notebook develops a model to predict customer purchase propensity based on demographic and behavioral features.\n",
    "\n",
    "## Objectives:\n",
    "- Predict likelihood of future purchases\n",
    "- Identify key drivers of purchase behavior\n",
    "- Segment customers by purchase potential\n",
    "- Provide targeting recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c188669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import pd_read_sql\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Snowflake\n",
    "conn_params = {\n",
    "    'user': 'workshop_user',\n",
    "    'password': 'VotreMotDePasse123!',\n",
    "    'account': 'dnb65599',\n",
    "    'warehouse': 'ANYCOMPANY_WH',\n",
    "    'database': 'ANYCOMPANY_LAB',\n",
    "    'schema': 'ANALYTICS'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b18d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data with synthetic target\n",
    "# Note: In real scenario, target would be based on actual purchase history\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    *,\n",
    "    -- Synthetic target: high propensity if high income and young age\n",
    "    CASE WHEN annual_income > 60000 AND age < 50 THEN 1 ELSE 0 END AS purchase_propensity\n",
    "FROM ANALYTICS.customer_ml_features\n",
    "\"\"\"\n",
    "\n",
    "df = pd_read_sql(query, conn)\n",
    "print(f\"Loaded {len(df)} customer records\")\n",
    "print(f\"Target distribution: {df['purchase_propensity'].value_counts(normalize=True)}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdf6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = ['age', 'annual_income', 'age_group_encoded', 'income_segment_encoded',\n",
    "           'region_north', 'region_south', 'region_east', 'region_west']\n",
    "\n",
    "X = df[features]\n",
    "y = df['purchase_propensity']\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Selected features: {features}\")\n",
    "print(f\"Feature matrix shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "lr_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Logistic Regression trained!\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_lr):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2367b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Random Forest trained!\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_proba_rf):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28f6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "models = ['Logistic Regression', 'Random Forest']\n",
    "predictions = [y_pred_lr, y_pred_rf]\n",
    "probabilities = [y_pred_proba_lr, y_pred_proba_rf]\n",
    "\n",
    "for name, pred, proba in zip(models, predictions, probabilities):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, proba):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for name, proba in zip(models, probabilities):\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Purchase Propensity Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ffa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (Random Forest)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance - Purchase Propensity')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "print(\"Top features driving purchase propensity:\")\n",
    "print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74549399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate propensity scores for all customers\n",
    "df['propensity_score'] = rf_model.predict_proba(scaler.transform(df[features]))[:, 1]\n",
    "\n",
    "# Create propensity segments\n",
    "df['propensity_segment'] = pd.qcut(df['propensity_score'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Propensity score distribution:\")\n",
    "print(df['propensity_segment'].value_counts().sort_index())\n",
    "\n",
    "# Visualize propensity distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='propensity_score', hue='propensity_segment', multiple='stack')\n",
    "plt.title('Customer Purchase Propensity Distribution')\n",
    "plt.xlabel('Propensity Score')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f259ae0a",
   "metadata": {},
   "source": [
    "## Business Recommendations\n",
    "\n",
    "### Key Insights:\n",
    "1. **Top Drivers**: Income and age are primary drivers of purchase propensity\n",
    "2. **High Propensity Segments**: Focus marketing efforts on high-propensity customers\n",
    "3. **Model Performance**: [ROC AUC scores]\n",
    "\n",
    "### Actionable Recommendations:\n",
    "1. **Targeted Marketing**: Prioritize high-propensity customers for campaigns\n",
    "2. **Personalization**: Tailor offers based on propensity scores\n",
    "3. **Retention Focus**: Develop retention strategies for high-propensity segments\n",
    "4. **Acquisition Strategy**: Target similar profiles for customer acquisition\n",
    "\n",
    "### Implementation:\n",
    "- Integrate propensity scores into CRM\n",
    "- Use scores for campaign targeting\n",
    "- Monitor score changes over time\n",
    "- Update model with new data quarterly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ddf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = df[['customer_id', 'propensity_score', 'propensity_segment']]\n",
    "print(f\"Results ready: {len(results_df)} customer propensity scores\")\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "print(\"Purchase propensity analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
